{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6674bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b92e11-154c-40d1-a9c2-29f1414099c5",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f749aab6-aa5d-4b1b-9211-7a7d04df8b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from trl import SFTTrainer\n",
    "from tools import prep_tokenizer, prepare_model\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ab16ae-a966-49d1-b967-10e9ccb5e2c2",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff3d8749-87e1-419b-9a38-d9dc5a669fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# elej√©re\n",
    "\n",
    "TESTING = True\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "MAX_SEQ_LENGTH = None\n",
    "QUANTIZE = True\n",
    "model_path = \"/mnt/shared/tibor/Llama-2-7b-chat-hf\"\n",
    "EPOCHS = 6\n",
    "# OUTPUT_DIR = f\"adapters/adapter_7b_4bit_quant_{EPOCHS}_epochs_noansw_no_modansw\"\n",
    "OUTPUT_DIR = f\"adapters/adapter_7b_4bit_quant_TEST\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5bb6dc-bdd3-4d31-b288-f95cfc729b61",
   "metadata": {},
   "source": [
    "## Prepare trainer and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576c27d3-8eee-41b8-a643-ae33fadab025",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = prep_tokenizer(model_path=model_path, add_eos_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43674b25-01da-4ece-ac6d-39958478657b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.model_max_length=1000000000000000019884624838656\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tokenizer.model_max_length=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e349ea9b-b0e7-4e8d-9a2b-8d02daf5f274",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.eos_token_id=2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 18817,  5524,   260, 23293,     2]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "print(f\"{tokenizer.eos_token_id=}\")\n",
    "inputs = tokenizer(\"Ez egy teszt\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].cuda()\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc2669cf-4083-4c29-bd2b-d8c5fee64541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63802e614cf4743a83602362178c363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = prepare_model(model_path=model_path, tokenizer=tokenizer, quantize=True, load_in_4bit=True, load_in_8bit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "380c4b93-8fcb-445b-90e8-6d26c7cd9c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=32,\n",
    "        # lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules= [\"q_proj\",\"v_proj\"]\n",
    "        # target_modules=target_modules_all_linear_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dbee705-eadb-4585-b1de-e5ceed3f4f59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_arguments=None\n",
    "if TESTING:\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=1,\n",
    "        per_device_eval_batch_size=4,\n",
    "        log_level=\"debug\",\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_steps=2, #change to 500, test: 2\n",
    "        logging_steps=1, #change to 100, test: 1\n",
    "        learning_rate=1e-4,\n",
    "        eval_steps=5, #change to 200, test: 5\n",
    "        bf16=True,\n",
    "        max_grad_norm=0.3,\n",
    "        # num_train_epochs=3, # remove \"#\"\n",
    "        max_steps=10, #remove this\n",
    "        warmup_ratio=0.03,\n",
    "        lr_scheduler_type=\"constant\",\n",
    ")\n",
    "else:\n",
    "    training_arguments = TrainingArguments(\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            do_eval=True,\n",
    "            per_device_train_batch_size=4,\n",
    "            gradient_accumulation_steps=1,\n",
    "            per_device_eval_batch_size=4,\n",
    "            log_level=\"debug\",\n",
    "            optim=\"paged_adamw_32bit\",\n",
    "            save_steps=500, #change to 500, test: 2\n",
    "            logging_steps=100, #change to 100, test: 1\n",
    "            learning_rate=1e-4,\n",
    "            eval_steps=200, #change to 200, test: 5\n",
    "            bf16=True,\n",
    "            max_grad_norm=0.3,\n",
    "            num_train_epochs=EPOCHS, # remove \"#\"\n",
    "            # max_steps=10, #remove this\n",
    "            warmup_ratio=0.03,\n",
    "            lr_scheduler_type=\"constant\",\n",
    "            report_to=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a319ead7-b751-4bcc-99da-fd22235b35fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", \n",
    "                       data_files={'train': 'data/train_w_noansw.csv', 'eval': 'data/eval_w_noansw.csv'},\n",
    "                       delimiter=\";\",\n",
    "                       column_names=['question', 'context', 'answer', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8851425-e029-4a53-afc2-7eb8f2dd2e03",
   "metadata": {},
   "source": [
    "## Check for longest inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2a85353-4157-4f5b-b2d8-62d64c4f106e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longest_embedding=985\n",
      "longest_embedding=965\n",
      "longest_embedding=965\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\", sep=';')\n",
    "eval_df = pd.read_csv(\"data/test.csv\", sep=';')\n",
    "test_df = pd.read_csv(\"data/test.csv\", sep=';')\n",
    "\n",
    "all_data=[train_df, eval_df, test_df]\n",
    "longest_embedding=0\n",
    "\n",
    "\n",
    "for df in all_data:\n",
    "    for index, row in df.loc[:].iterrows():\n",
    "\n",
    "        inputs = tokenizer(row[\"text\"], return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].cuda()\n",
    "        \n",
    "        if len(input_ids[0]) > longest_embedding:\n",
    "            longest_embedding = len(input_ids[0])\n",
    "\n",
    "    print(f\"{longest_embedding=}\")\n",
    "    longest_embedding=0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6870a3-f460-4cec-bee3-eeff84ed2f2a",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2d00715-aceb-49f6-895b-baccf552e679",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/shared/tibor/miniconda3/envs/test/lib/python3.10/site-packages/peft/utils/other.py:133: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/mnt/shared/tibor/miniconda3/envs/test/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:207: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "[codecarbon INFO @ 13:45:21] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 13:45:22] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 13:45:22] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 13:45:22] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 13:45:22] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 13:45:24] We saw that you have a AMD EPYC-Rome Processor but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 13:45:24] CPU Model on constant consumption mode: AMD EPYC-Rome Processor\n",
      "[codecarbon INFO @ 13:45:24] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 13:45:24]   Platform system: Linux-5.15.0-41-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 13:45:24]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 13:45:24]   CodeCarbon version: 2.2.3\n",
      "[codecarbon INFO @ 13:45:24]   Available RAM : 31.354 GB\n",
      "[codecarbon INFO @ 13:45:24]   CPU count: 8\n",
      "[codecarbon INFO @ 13:45:24]   CPU model: AMD EPYC-Rome Processor\n",
      "[codecarbon INFO @ 13:45:24]   GPU count: 1\n",
      "[codecarbon INFO @ 13:45:24]   GPU model: 1 x GRID A100-20C\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['eval'],\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=min(tokenizer.model_max_length, 1024),  # default: min(tokenizer.model_max_length, 1024),\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63645be6-4232-4538-8483-ac0c0559a2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 4\n",
      "***** Running training *****\n",
      "  Num examples = 3,188\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "  Number of trainable parameters = 4,194,304\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 02:55, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.866600</td>\n",
       "      <td>1.918341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.704900</td>\n",
       "      <td>1.750651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to adapters/adapter_7b_4bit_quant_TEST/checkpoint-2\n",
      "tokenizer config file saved in adapters/adapter_7b_4bit_quant_TEST/checkpoint-2/tokenizer_config.json\n",
      "Special tokens file saved in adapters/adapter_7b_4bit_quant_TEST/checkpoint-2/special_tokens_map.json\n",
      "Saving model checkpoint to adapters/adapter_7b_4bit_quant_TEST/checkpoint-4\n",
      "tokenizer config file saved in adapters/adapter_7b_4bit_quant_TEST/checkpoint-4/tokenizer_config.json\n",
      "Special tokens file saved in adapters/adapter_7b_4bit_quant_TEST/checkpoint-4/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 684\n",
      "  Batch size = 4\n",
      "[codecarbon INFO @ 13:45:40] Energy consumed for RAM : 0.000049 kWh. RAM Power : 11.757657051086426 W\n",
      "[codecarbon INFO @ 13:45:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 13:45:40] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:45:40] 0.000226 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:45:55] Energy consumed for RAM : 0.000098 kWh. RAM Power : 11.757657051086426 W\n",
      "[codecarbon INFO @ 13:45:55] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 13:45:55] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:45:55] 0.000452 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:46:10] Energy consumed for RAM : 0.000147 kWh. RAM Power : 11.757657051086426 W\n",
      "[codecarbon INFO @ 13:46:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 13:46:10] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:46:10] 0.000678 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:46:25] Energy consumed for RAM : 0.000196 kWh. RAM Power : 11.757657051086426 W\n",
      "[codecarbon INFO @ 13:46:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 13:46:25] Energy consumed for all CPUs : 0.000708 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:46:25] 0.000904 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:46:40] Energy consumed for RAM : 0.000245 kWh. RAM Power : 11.757657051086426 W\n",
      "[codecarbon INFO @ 13:46:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 13:46:40] Energy consumed for all CPUs : 0.000885 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:46:40] 0.001130 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:46:55] Energy consumed for RAM : 0.000294 kWh. RAM Power : 11.757657051086426 W\n",
      "[codecarbon INFO @ 13:46:55] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 13:46:55] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:46:55] 0.001356 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to adapters/adapter_7b_4bit_quant_TEST/checkpoint-6\n",
      "tokenizer config file saved in adapters/adapter_7b_4bit_quant_TEST/checkpoint-6/tokenizer_config.json\n",
      "Special tokens file saved in adapters/adapter_7b_4bit_quant_TEST/checkpoint-6/special_tokens_map.json\n",
      "Saving model checkpoint to adapters/adapter_7b_4bit_quant_TEST/checkpoint-8\n",
      "tokenizer config file saved in adapters/adapter_7b_4bit_quant_TEST/checkpoint-8/tokenizer_config.json\n",
      "Special tokens file saved in adapters/adapter_7b_4bit_quant_TEST/checkpoint-8/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 684\n",
      "  Batch size = 4\n",
      "[codecarbon INFO @ 13:47:10] Energy consumed for RAM : 0.000343 kWh. RAM Power : 11.757657051086426 W\n",
      "[codecarbon INFO @ 13:47:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 13:47:10] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:47:10] 0.001582 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:47:25] Energy consumed for RAM : 0.000392 kWh. RAM Power : 11.757657051086426 W\n",
      "[codecarbon INFO @ 13:47:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 13:47:25] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:47:25] 0.001808 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:47:40] Energy consumed for RAM : 0.000441 kWh. RAM Power : 11.757657051086426 W\n",
      "[codecarbon INFO @ 13:47:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 13:47:40] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:47:40] 0.002034 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:47:55] Energy consumed for RAM : 0.000490 kWh. RAM Power : 11.757657051086426 W\n",
      "[codecarbon INFO @ 13:47:55] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 13:47:55] Energy consumed for all CPUs : 0.001771 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:47:55] 0.002261 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:48:10] Energy consumed for RAM : 0.000539 kWh. RAM Power : 11.757657051086426 W\n",
      "[codecarbon INFO @ 13:48:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 13:48:10] Energy consumed for all CPUs : 0.001948 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:48:10] 0.002487 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:48:25] Energy consumed for RAM : 0.000588 kWh. RAM Power : 11.757657051086426 W\n",
      "[codecarbon INFO @ 13:48:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 13:48:25] Energy consumed for all CPUs : 0.002125 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:48:25] 0.002713 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to adapters/adapter_7b_4bit_quant_TEST/checkpoint-10\n",
      "tokenizer config file saved in adapters/adapter_7b_4bit_quant_TEST/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in adapters/adapter_7b_4bit_quant_TEST/checkpoint-10/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[codecarbon INFO @ 13:48:28] Energy consumed for RAM : 0.000596 kWh. RAM Power : 11.757657051086426 W\n",
      "[codecarbon INFO @ 13:48:28] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 13:48:28] Energy consumed for all CPUs : 0.002156 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:48:28] 0.002753 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=1.9409354090690614, metrics={'train_runtime': 182.6829, 'train_samples_per_second': 0.219, 'train_steps_per_second': 0.055, 'total_flos': 1017129075671040.0, 'train_loss': 1.9409354090690614, 'epoch': 0.01})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf2519-3c5d-41d2-ae80-3657c77cf0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
