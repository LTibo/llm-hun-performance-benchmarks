{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "135e0c0e-dacd-49d7-8f6b-610e7725e142",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b5b403-3ac6-467b-84ec-b205cc2deb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    GenerationConfig,\n",
    "    LogitsProcessor\n",
    ")\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ad8c23-f192-4fd4-af5e-b4315b5149e8",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f04a090-a376-41c0-bae0-78d2859b129f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"/mnt/shared/tibor/Llama-2-7b-chat-hf\"\n",
    "NOT_ANSWERABLE_INCLUDED = True\n",
    "MAX_NEW_TOKENS = 256\n",
    "OUTPUT_FILE = \"test_llama_7b_finetuned_not_all_answerable_4bit\"\n",
    "USE_ADAPTER = True\n",
    "ADAPTER = \"./adapters/adapter_7b_4bit_quant_6_epochs_noansw_no_modansw/checkpoint-4000\"\n",
    "QUANTIZE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2cff34-015b-46ad-8b5c-9a150706e084",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ba82a6-8418-4bd3-ba8d-d2942bf3377a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EosTokenRewardLogitsProcessor(LogitsProcessor):\n",
    "  def __init__(self,  eos_token_id: int, max_length: int):\n",
    "    \n",
    "        if not isinstance(eos_token_id, int) or eos_token_id < 0:\n",
    "            raise ValueError(f\"`eos_token_id` has to be a positive integer, but is {eos_token_id}\")\n",
    "\n",
    "        if not isinstance(max_length, int) or max_length < 1:\n",
    "          raise ValueError(f\"`max_length` has to be a integer bigger than 1, but is {max_length}\")\n",
    "\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.max_length=max_length\n",
    "\n",
    "  def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    cur_len = input_ids.shape[-1]\n",
    "    # start to increese the reward of the  eos_tokekn from 80% max length  progressively on length\n",
    "    for cur_len in (max(0,int(self.max_length*0.8)), self.max_length ):\n",
    "      ratio = cur_len/self.max_length\n",
    "      num_tokens = scores.shape[1] # size of vocab\n",
    "      scores[:, [i for i in range(num_tokens) if i != self.eos_token_id]] =\\\n",
    "      scores[:, [i for i in range(num_tokens) if i != self.eos_token_id]]*ratio*10*torch.exp(-torch.sign(scores[:, [i for i in range(num_tokens) if i != self.eos_token_id]]))\n",
    "      scores[:, self.eos_token_id] = 1e2*ratio\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "889c8c28-c787-4e69-baae-6df8f979148a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "#Create a new token and add it to the tokenizer\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f62ff60-6ad5-48d0-be5f-dca6efe1c235",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86c3dded3664109bbdcd68fc7148ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        # load_in_8bit=True,\n",
    "        # bnb_8bit_quant_type=\"nf8\",\n",
    "        # bnb_8bit_compute_dtype=\"bfloat16\",\n",
    "        # bnb_8bit_use_double_quant=True,\n",
    "        \n",
    ")\n",
    "\n",
    "if QUANTIZE:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "              model_path, quantization_config=bnb_config, device_map={\"\": 0}\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "              model_path,\n",
    "              torch_dtype=torch.bfloat16,\n",
    "              device_map={\"\": 0}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40364437-7956-4a10-bca6-2e54b60aed28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32016, 4096)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Resize the embeddings\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=16) # https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de045f30-62a9-4881-a45b-23e54780075d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False # Gradient checkpointing is used by default but not compatible with caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e258673-4ff4-439e-b3d7-eade79520c03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ./adapters/adapter_7b_4bit_quant_6_epochs_noansw_no_modansw/checkpoint-4000 as adapter\n"
     ]
    }
   ],
   "source": [
    "if USE_ADAPTER:\n",
    "    print(\"Using \" + ADAPTER + \" as adapter\")\n",
    "    model = PeftModel.from_pretrained(model, ADAPTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ad47a38-df75-4d3d-a0c5-d23578b336cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "impossible_answer = \"Nincs elegendő adat a kérdés megválaszolásához.\"\n",
    "prompt_prefix = None\n",
    "\n",
    "if NOT_ANSWERABLE_INCLUDED:\n",
    "    prompt_prefix = (\"Válaszold meg az alábbi kérdést a forrás alapján! \"\n",
    "                     \"Ha a forrás alapján nem megválaszolható a kérdés, mondd hogy: \"\n",
    "                     f\"{impossible_answer}\")\n",
    "else:\n",
    "    prompt_prefix = (\"Válaszold meg az alábbi kérdést a forrás alapján!\")\n",
    "\n",
    "\n",
    "def generate(context: str, question: str, use_source: bool = True):\n",
    "    prompt = None\n",
    "    if use_source:\n",
    "        prompt = (prompt_prefix\n",
    "                  + \"\\n### Forrás:\\n\" + str(context)\n",
    "                  + \"\\n### Kérdés:\\n\" + str(question)\n",
    "                  + \"\\n### Válasz:\\n\")\n",
    "    else:\n",
    "        prompt = (\"Válaszold meg az alábbi kérdést\"\n",
    "                 + \"\\n### Kérdés:\\n\" + str(question)\n",
    "                 + \"\\n### Válasz:\\n\")\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "\n",
    "    # print(f\"{len(input_ids[0])=}\")\n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        do_sample=True,\n",
    "        # top_p=1.0,\n",
    "        # top_k=50,\n",
    "        # num_beams=1,\n",
    "        temperature=0.2,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    generation_output = model.generate(\n",
    "            # logits_processor=[EosTokenRewardLogitsProcessor(eos_token_id=tokenizer.eos_token_id, max_length = 64)],\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config)\n",
    "\n",
    "    for seq in generation_output.sequences:\n",
    "        output = tokenizer.decode(seq)\n",
    "        return output.split(\"### Válasz:\\n\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1f9f647-eeaa-4ded-b731-0e397b6b3fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "683"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"data/test_w_noansw.csv\", sep=';')\n",
    "test_df_len = len(test_df)\n",
    "test_count = int(test_df_len)\n",
    "# test_count = 10\n",
    "test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b02aed-a90e-4441-80c0-4517882cc6f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inferences:  75%|██████████████████████████████████████████▊              | 513/683 [2:04:32<1:00:08, 21.23s/it]"
     ]
    }
   ],
   "source": [
    "results = {\"reference\": [],\n",
    "           \"question\": [],\n",
    "           \"correct_answer\": [],\n",
    "           \"answer_w_ref\": [],\n",
    "           \"answer_no_ref\": []}\n",
    "\n",
    "with tqdm(total=test_count, desc=\"Running inferences\") as pbar:\n",
    "    for index, row in test_df.iloc[:test_count].iterrows():\n",
    "\n",
    "        answer_w_ref = generate(context=row['context'], question=row['question'])\n",
    "        answer_no_ref = generate(context=row['context'], question=row['question'], use_source=False)\n",
    "\n",
    "        results[\"reference\"].append(row[\"context\"])\n",
    "        results[\"question\"].append(row[\"question\"])\n",
    "        results[\"correct_answer\"].append(row[\"answer\"])\n",
    "        results[\"answer_w_ref\"].append(answer_w_ref)\n",
    "        results[\"answer_no_ref\"].append(answer_no_ref)\n",
    "\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00baa736-67fb-4cb8-bb17-5ab8802defda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df=pd.DataFrame(results)\n",
    "results_df.to_csv(\"results/\"+OUTPUT_FILE+\".csv\", sep=';')\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec39cc9-1b51-4ce8-84a0-67fe6779863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f892757e-ded7-4fbf-a0d8-1ccc5ca28206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
